{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a653e2d6",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29c2c146",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anike\\miniconda3\\envs\\DS\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545c99f6",
   "metadata": {},
   "source": [
    "# CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b6b845d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"mbpp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49f0f5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train']['code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd026a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "        num_rows: 374\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "        num_rows: 500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "        num_rows: 90\n",
       "    })\n",
       "    prompt: Dataset({\n",
       "        features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcc45ca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 251820/251820 [00:01<00:00, 158289.48 examples/s]\n",
      "Generating validation split: 100%|██████████| 13914/13914 [00:00<00:00, 130275.57 examples/s]\n",
      "Generating test split: 100%|██████████| 14918/14918 [00:00<00:00, 156124.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"google/code_x_glue_ct_code_to_text\", \"python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95128c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n",
       "        num_rows: 251820\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n",
       "        num_rows: 13914\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'repo', 'path', 'func_name', 'original_string', 'language', 'code', 'code_tokens', 'docstring', 'docstring_tokens', 'sha', 'url'],\n",
       "        num_rows: 14918\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bf301d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anike\\miniconda3\\envs\\DS\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "123362a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/raw/task_1_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbb40a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.3\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed707bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(row, task):\n",
    "    return row['text'], row['code']\n",
    "    # prompt = x[\"text\"]\n",
    "    # answer = x[\"code\"]\n",
    "\n",
    "    # prompt, answer = formatted.split(\"### Response:\\n\", 1)\n",
    "    # prompt_tokens = tokenizer(prompt, truncation=False, add_special_tokens=False)\n",
    "    # answer_tokens = tokenizer(answer, truncation=False, add_special_tokens=False)\n",
    "\n",
    "    # input_ids = prompt_tokens[\"input_ids\"] + answer_tokens[\"input_ids\"]\n",
    "    # attention_mask = [1] * len(input_ids)\n",
    "    # labels = [-100] * len(prompt_tokens[\"input_ids\"]) + answer_tokens[\"input_ids\"]\n",
    "\n",
    "    # # Pad up to MAX_LENGTH\n",
    "    # padding_length = MAX_LENGTH - len(input_ids)\n",
    "    # if padding_length > 0:\n",
    "    #     input_ids += [tokenizer.pad_token_id] * padding_length\n",
    "    #     attention_mask += [0] * padding_length\n",
    "    #     labels += [-100] * padding_length\n",
    "    # else:\n",
    "    #     input_ids = input_ids[:MAX_LENGTH]\n",
    "    #     attention_mask = attention_mask[:MAX_LENGTH]\n",
    "    #     labels = labels[:MAX_LENGTH]\n",
    "\n",
    "    # return {\n",
    "    #     \"input_ids\": input_ids,\n",
    "    #     \"attention_mask\": attention_mask,\n",
    "    #     \"labels\": labels\n",
    "    # }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e61423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map each row of the dataframe to a string\n",
    "d1 = dataset.apply(lambda row: preprocess_data(row, \"task_1\"), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9be4c14a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Write a function to find the longest chain which can be formed from the given set of pairs.',\n",
       " 'class Pair(object): \\r\\n\\tdef __init__(self, a, b): \\r\\n\\t\\tself.a = a \\r\\n\\t\\tself.b = b \\r\\ndef max_chain_length(arr, n): \\r\\n\\tmax = 0\\r\\n\\tmcl = [1 for i in range(n)] \\r\\n\\tfor i in range(1, n): \\r\\n\\t\\tfor j in range(0, i): \\r\\n\\t\\t\\tif (arr[i].a > arr[j].b and\\r\\n\\t\\t\\t\\tmcl[i] < mcl[j] + 1): \\r\\n\\t\\t\\t\\tmcl[i] = mcl[j] + 1\\r\\n\\tfor i in range(n): \\r\\n\\t\\tif (max < mcl[i]): \\r\\n\\t\\t\\tmax = mcl[i] \\r\\n\\treturn max')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2b66983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89ce2498",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_1_train_data = load_from_disk('../data/split/task_1_train_data_hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6b0e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_1_train_data['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ace04071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf06e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "ADAPTERS_DIR = \"../saved_models/adapters\"\n",
    "TASK_1_ADAPTER = os.path.join(ADAPTERS_DIR, \"task_1\")\n",
    "TASK_2_ADAPTER = os.path.join(ADAPTERS_DIR, \"task_2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342023a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
